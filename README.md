Human musicians are experimenting with jambots, which are systems that use deep neural nets to generate music in real-time. One challenge with generative-AI music is the lack of communication between the human musician and the AI. 
When people improvise music, they communicate physically and visually in real-time. This enhances the music-making process. 
With current jambots, this sort of collaboration is hard to recreate because of the lack of feedback between the jambot and the user. 
My project aims to solve this problem. This music visualization app is a graphical animation system that shows what the jambot is “thinking” before it plays. 
The visualization aims to answer the following questions: 
- What is the AI playing and why?
- How is the AI reacting to the user in real-time?

The app is currently just on localhost. npm run dev to test it out yourself!
